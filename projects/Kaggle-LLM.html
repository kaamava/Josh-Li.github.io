<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="title" content="Kaggle-LLM | Josh Li | Professional Portfolio">
    <meta name="description" content="">
    <meta name="image" content="https://github.com/kaamava.png">
    <meta property="og:type" content="website">
    <meta property="og:site_name" content="Josh Li | Professional Portfolio">
    <meta property="og:url" content="https://kaamava.github.io/projects/Kaggle-LLM.html">
    <meta property="og:title" content="Kaggle-LLM | Josh Li | Professional Portfolio">
    <meta property="og:description" content="">
    <meta property="og:image" content="https://github.com/kaamava.png">
    <link rel="shortcut icon" href="/favicon.ico">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.0-beta1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-0evHe/X+R7YkIZDRvuzKMRqM+OrBnVFBL6DOitfPri4tjfHxaWutUpFmBp4vmVor" crossorigin="anonymous">
    <link rel="stylesheet" href="/css/techfolio-theme/skyblue.css">
    <link rel="stylesheet" type="text/css" href="/css/rouge/github.css">
    <!-- Load MathJax if 'mathjax: true' is found in your _config.yml. -->
    
    <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-MML-AM_CHTML">
    </script>
    

    <title>Kaggle-LLM | Josh Li | Professional Portfolio</title>
  </head>
  <body>
  <header class="navbar navbar-expand navbar-light bg-light bg-gradient border-bottom">
  <div class="container-fluid">
    <a class="navbar-brand" href="/">Josh Li</a>
    <div class="ms-auto">
      <ul class="navbar-nav mb-2 mb-lg-0">
        <a class="nav-link" href="/#projects">Projects</a>
        <a class="nav-link" href="/#essays">Essays</a>
        <a class="nav-link" href="/resume.html">Resume</a>
      </ul>
    </div>
  </div>
</header>

<div class="container py-4">
  <h1 class="display-4">Kaggle-LLM</h1>
 <h1 id="kaggle-llm-comp">kaggle-LLM-Comp</h1>
<h3 id="goal-of-the-competition">Goal of the Competition</h3>

<p>Inspired by the OpenBookQA dataset, this competition challenges participants to answer difficult science-based questions written by a Large Language Model.</p>

<p>Your work will help researchers better understand the ability of LLMs to test themselves, and the potential of LLMs that can be run in resource-constrained environments.</p>

<h3 id="context">Context</h3>
<p>As the scope of large language model capabilities expands, a growing area of research is using LLMs to characterize themselves. Because many preexisting NLP benchmarks have been shown to be trivial for state-of-the-art models, there has also been interesting work showing that LLMs can be used to create more challenging tasks to test ever more powerful models.</p>

<p>At the same time methods like quantization and knowledge distillation are being used to effectively shrink language models and run them on more modest hardware. The Kaggle environment provides a unique lens to study this as submissions are subject to both GPU and time limits.</p>

<p>The dataset for this challenge was generated by giving gpt3.5 snippets of text on a range of scientific topics pulled from wikipedia, and asking it to write a multiple choice question (with a known answer), then filtering out easy questions.</p>

<p>Right now we estimate that the largest models run on Kaggle are around 10 billion parameters, whereas gpt3.5 clocks in at 175 billion parameters. If a question-answering model can ace a test written by a question-writing model more than 10 times its size, this would be a genuinely interesting result; on the other hand if a larger model can effectively stump a smaller one, this has compelling implications on the ability of LLMs to benchmark and test themselves.</p>

<h3 id="project-introduction">Project Introduction</h3>

<p>LLM_train.ipynb  Model training code</p>

<p>LLM_infer.ipynb  Model infering code</p>

<p>llama_finetune.ipynb  llama model fine-tuning reference</p>

<p>Note: It is recommended to use GPU A100 to train the model. The model training time is approximately 6 hours.</p>

<p>Download open source data source(required):
https://www.kaggle.com/datasets/cdeotte/60k-data-with-context-v2</p>

<h3 id="our-advantages">Our Advantages</h3>
<ol>
  <li>
    <p>This competition is based on Wikipedia reading comprehension data and aims to develop a model for automatically answering multiple-choice questions. It is a typical multi-class classification problem, and to evaluate the accuracy of prediction results, the competition uses MAP@3 as the evaluation metric.</p>
  </li>
  <li>
    <p>This competition provides correct answers for only 200 questions and does not provide reference articles. Therefore, participants need to search for additional training data to achieve better model performance. We generated 60,000 training samples by using the original Wikipedia texts and the GPT-3.5 model.</p>
  </li>
  <li>
    <p>Since the competition does not provide the original texts, which contain a significant amount of useful information, we adopted a sentence vector approach. We measured the similarity between the prompt and the original Wikipedia texts to extract the most similar original text, which serves as the primary reference for the questions.</p>
  </li>
  <li>
    <p>Based on the original text, prompt, options, and correct answers, we encoded the input using DeBERTa-v3-large and built a reading comprehension model using the AutoModelForMultipleChoice model from the Transformers library. When our model answers multiple-choice questions automatically, we first match the prompt and options with the original text and then use the model to predict the correct answer. Our model achieved a MAP@3 of 0.906 on the test set and ranked in the top 7% in the competition.</p>
  </li>
  <li>
    <p>During the competition, we made efficient use of the competition time, controlled the pace effectively, engaged in active discussions, and formulated reasonable tasks for model iteration. We also organized brainstorming sessions to drive the progress of the competition. Meanwhile, we worked on model development to ensure the highest possible prediction accuracy.
<img src="https://github.com/kaamava/kaggle-LLM-Comp/assets/106901273/cbe3ae98-8264-47ae-9b9f-b5dd80a45bf8" alt="image" /></p>
  </li>
</ol>

<p><img src="https://github.com/kaamava/kaggle-LLM-Comp/assets/106901273/fd97ba12-2b3a-4c08-bba6-8a3a385ccf65" alt="image" /></p>
<hr />

<p>Source: <a href="https://github.com/kaamava/kaggle-LLM-Comp"><i class="large github icon "></i>Kaggle-LLM</a></p>

</div>

<footer class="navbar navbar-expand navbar-light bg-light bg-gradient border-top">
  <div class="container-fluid">
    <div class="ms-auto">
      <ul class="navbar-nav mb-2 mb-lg-0">
        <small><a class="nav-link" href="https://techfolios.github.io">Made with Techfolios</a></small>
      </ul>
    </div>
  </div>
</footer>


  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.0-beta1/dist/js/bootstrap.bundle.min.js" integrity="sha384-pprn3073KE6tl6bjs2QrFaJGz5/SUsLqktiwsUTF55Jfv3qYSDhgCecCxMW52nD2" crossorigin="anonymous"></script>
  </body>
</html>
